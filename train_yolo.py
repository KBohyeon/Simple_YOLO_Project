# train_yolo.py - YOLO ëª¨ë¸ í›ˆë ¨

import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import time
import argparse
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

from yolo_model import SimpleYOLO, YOLOLoss, RealImageDataset, get_transform, save_model
from config import *

class YOLOTrainer:
    """YOLO í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, image_folder, config=None):
        self.image_folder = Path(image_folder)
        self.config = config or TRAIN_CONFIG
        self.device = DEVICE
        
        # ëª¨ë¸ ë° ë°ì´í„° ì´ˆê¸°í™”
        self.setup_model()
        self.setup_data()
        
        # í›ˆë ¨ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        
    def setup_model(self):
        """ëª¨ë¸ ë° ìµœì í™” ì„¤ì •"""
        print("ğŸ§  ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ëª¨ë¸
        self.model = SimpleYOLO(
            num_classes=MODEL_CONFIG['num_classes'],
            grid_size=MODEL_CONFIG['grid_size']
        ).to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.criterion = YOLOLoss(
            lambda_coord=self.config['lambda_coord'],
            lambda_noobj=self.config['lambda_noobj']
        )
        
        # ìµœì í™”ê¸°
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=self.config['epochs']//3,
            gamma=0.1
        )
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        print(f"   - í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        
    def setup_data(self):
        """ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì„¤ì •"""
        print("ğŸ“Š ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„° ë³€í™˜
        train_transform = get_transform(train=True)
        val_transform = get_transform(train=False)
        
        # ì „ì²´ ë°ì´í„°ì…‹
        full_dataset = RealImageDataset(
            self.image_folder,
            grid_size=MODEL_CONFIG['grid_size'],
            img_size=MODEL_CONFIG['img_size'],
            transform=train_transform
        )
        
        if len(full_dataset) == 0:
            raise ValueError(f"âŒ ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {self.image_folder}")
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        train_size = int(self.config['train_val_split'] * len(full_dataset))
        val_size = len(full_dataset) - train_size
        
        self.train_dataset, val_dataset = random_split(
            full_dataset, [train_size, val_size]
        )
        
        # ê²€ì¦ ë°ì´í„°ì…‹ì— ë‹¤ë¥¸ ë³€í™˜ ì ìš©
        val_dataset.dataset = RealImageDataset(
            self.image_folder,
            grid_size=MODEL_CONFIG['grid_size'],
            img_size=MODEL_CONFIG['img_size'],
            transform=val_transform
        )
        
        # ë°ì´í„°ë¡œë”
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        print(f"   - ì´ ë°ì´í„°: {len(full_dataset)}ê°œ")
        print(f"   - í›ˆë ¨ ë°ì´í„°: {len(self.train_dataset)}ê°œ")
        print(f"   - ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.config['batch_size']}")
        
    def train_epoch(self, epoch):
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        total_loss = 0
        num_batches = len(self.train_loader)
        
        print(f"\nì—í¬í¬ {epoch+1}/{self.config['epochs']} í›ˆë ¨ ì¤‘...")
        
        for batch_idx, (images, targets) in enumerate(self.train_loader):
            images, targets = images.to(self.device), targets.to(self.device)
            
            # ìˆœì „íŒŒ
            self.optimizer.zero_grad()
            outputs = self.model(images)
            loss = self.criterion(outputs, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì„ íƒì )
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if batch_idx % max(1, num_batches // 10) == 0:
                progress = (batch_idx + 1) / num_batches * 100
                print(f"   ë°°ì¹˜ {batch_idx+1}/{num_batches} ({progress:.1f}%) - "
                      f"ì†ì‹¤: {loss.item():.4f}")
        
        avg_loss = total_loss / num_batches
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    def validate_epoch(self, epoch):
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        num_batches = len(self.val_loader)
        
        print(f"ì—í¬í¬ {epoch+1} ê²€ì¦ ì¤‘...")
        
        with torch.no_grad():
            for images, targets in self.val_loader:
                images, targets = images.to(self.device), targets.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, targets)
                
                total_loss += loss.item()
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        self.val_losses.append(avg_loss)
        
        return avg_loss
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        PATHS['models_dir'].mkdir(parents=True, exist_ok=True)
        
        if is_best:
            save_path = PATHS['models_dir'] / 'best_yolo_model.pth'
            save_model(self.model, self.optimizer, epoch, val_loss, save_path)
            print(f"âœ… ìµœê³  ëª¨ë¸ ì €ì¥: {save_path}")
        
        # ë§ˆì§€ë§‰ ëª¨ë¸ë„ ì €ì¥
        last_save_path = PATHS['models_dir'] / 'last_yolo_model.pth'
        save_model(self.model, self.optimizer, epoch, val_loss, last_save_path)
    
    def plot_training_history(self):
        """í›ˆë ¨ ê³¼ì • ì‹œê°í™”"""
        plt.figure(figsize=(12, 5))
        
        # ì†ì‹¤ ê·¸ë˜í”„
        plt.subplot(1, 2, 1)
        epochs = range(1, len(self.train_losses) + 1)
        plt.plot(epochs, self.train_losses, 'b-', label='í›ˆë ¨ ì†ì‹¤', linewidth=2)
        if self.val_losses and max(self.val_losses) > 0:
            plt.plot(epochs, self.val_losses, 'r-', label='ê²€ì¦ ì†ì‹¤', linewidth=2)
        plt.xlabel('ì—í¬í¬')
        plt.ylabel('ì†ì‹¤')
        plt.title('í›ˆë ¨ ê³¼ì •')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # í•™ìŠµë¥  ê·¸ë˜í”„
        plt.subplot(1, 2, 2)
        lrs = [group['lr'] for group in self.optimizer.param_groups]
        plt.plot(epochs, [lrs[0]] * len(epochs), 'g-', linewidth=2)
        plt.xlabel('ì—í¬í¬')
        plt.ylabel('í•™ìŠµë¥ ')
        plt.title('í•™ìŠµë¥  ë³€í™”')
        plt.grid(True, alpha=0.3)
        plt.yscale('log')
        
        plt.tight_layout()
        
        # ì €ì¥
        plot_path = PATHS['models_dir'] / 'training_history.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š í›ˆë ¨ ê·¸ë˜í”„ ì €ì¥: {plot_path}")
    
    def train(self):
        """ì „ì²´ í›ˆë ¨ ê³¼ì •"""
        print("=" * 70)
        print("ğŸš€ YOLO ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        print("=" * 70)
        
        start_time = time.time()
        
        try:
            for epoch in range(self.config['epochs']):
                print(f"\n{'='*50}")
                
                # í›ˆë ¨
                train_loss = self.train_epoch(epoch)
                
                # ê²€ì¦
                val_loss = self.validate_epoch(epoch)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                self.scheduler.step()
                
                # ê²°ê³¼ ì¶œë ¥
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"\nì—í¬í¬ {epoch+1} ì™„ë£Œ:")
                print(f"   - í›ˆë ¨ ì†ì‹¤: {train_loss:.4f}")
                print(f"   - ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
                print(f"   - í•™ìŠµë¥ : {current_lr:.6f}")
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                is_best = val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_loss
                
                self.save_checkpoint(epoch, val_loss, is_best)
                
                # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ (ì„ íƒì )
                if train_loss < 0.01:
                    print(f"\nğŸ¯ í›ˆë ¨ ì†ì‹¤ì´ ì¶©ë¶„íˆ ë‚®ì•„ì¡ŒìŠµë‹ˆë‹¤ ({train_loss:.4f})")
                    break
        
        except KeyboardInterrupt:
            print("\nâš ï¸ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
        
        finally:
            # í›ˆë ¨ ì™„ë£Œ
            end_time = time.time()
            training_time = end_time - start_time
            
            print("\n" + "=" * 70)
            print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
            print("=" * 70)
            print(f"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {training_time//60:.0f}ë¶„ {training_time%60:.1f}ì´ˆ")
            print(f"ğŸ† ìµœê³  ê²€ì¦ ì†ì‹¤: {self.best_val_loss:.4f}")
            print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {PATHS['models_dir']}")
            
            # í›ˆë ¨ ê·¸ë˜í”„ ìƒì„±
            self.plot_training_history()
            
            # í›ˆë ¨ í†µê³„ ì €ì¥
            self.save_training_stats(training_time)
    
    def save_training_stats(self, training_time):
        """í›ˆë ¨ í†µê³„ ì €ì¥"""
        stats = {
            'config': self.config,
            'model_config': MODEL_CONFIG,
            'training_time': training_time,
            'best_val_loss': self.best_val_loss,
            'final_train_loss': self.train_losses[-1] if self.train_losses else None,
            'epochs_completed': len(self.train_losses),
            'dataset_size': len(self.train_dataset) + len(self.val_loader.dataset),
            'classes': CLASSES
        }
        
        import json
        stats_path = PATHS['models_dir'] / 'training_stats.json'
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š í›ˆë ¨ í†µê³„ ì €ì¥: {stats_path}")

def check_dataset(image_folder):
    """ë°ì´í„°ì…‹ ìƒíƒœ í™•ì¸"""
    from config import PATHS, CLASSES
    
    image_folder = Path(image_folder)
    annotations_dir = PATHS['annotations_dir']  # â† í•µì‹¬ ì¶”ê°€!
    
    if not image_folder.exists():
        print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {image_folder}")
        return False
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
        image_files.extend(image_folder.glob(ext))
        image_files.extend(image_folder.glob(ext.upper()))
    
    # ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ ì°¾ê¸° (annotations í´ë”ì—ì„œ!)
    labeled_files = []
    for img_file in image_files:
        annotation_path = annotations_dir / f"{img_file.stem}.json"  # â† í•µì‹¬ ìˆ˜ì •!
        if annotation_path.exists():
            labeled_files.append((img_file, annotation_path))
    
    print(f"ğŸ“Š ë°ì´í„°ì…‹ í˜„í™©:")
    print(f"   - ì „ì²´ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
    print(f"   - ë¼ë²¨ë§ëœ ì´ë¯¸ì§€: {len(labeled_files)}ê°œ")
    
    if len(labeled_files) == 0:
        print("âŒ ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("ë¨¼ì € labeling_tool.pyë¥¼ ì‹¤í–‰í•´ì„œ ì´ë¯¸ì§€ë“¤ì„ ë¼ë²¨ë§í•˜ì„¸ìš”.")
        return False
    
    # í´ë˜ìŠ¤ë³„ í†µê³„
    class_counts = {cls: 0 for cls in CLASSES}
    total_objects = 0
    
    for img_file, json_file in labeled_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:  # â† ì¸ì½”ë”© ì¶”ê°€!
                data = json.load(f)
                for ann in data.get('annotations', []):
                    class_name = ann.get('class_name', '')
                    if class_name in class_counts:
                        class_counts[class_name] += 1
                        total_objects += 1
        except:
            continue
    
    print(f"   - ì´ ê°ì²´ ìˆ˜: {total_objects}ê°œ")
    print(f"   - í´ë˜ìŠ¤ë³„ ë¶„í¬:")
    for class_name, count in class_counts.items():
        print(f"     â€¢ {class_name}: {count}ê°œ")
    
    # ê¶Œì¥ì‚¬í•­
    min_per_class = 20
    insufficient_classes = [cls for cls, count in class_counts.items() if count < min_per_class]
    
    if insufficient_classes:
        print(f"\nâš ï¸ ê¶Œì¥ì‚¬í•­: ë‹¤ìŒ í´ë˜ìŠ¤ë“¤ì˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ {min_per_class}ê°œ ê¶Œì¥):")
        for cls in insufficient_classes:
            print(f"   - {cls}: {class_counts[cls]}ê°œ")
    
    return len(labeled_files) > 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='YOLO ëª¨ë¸ í›ˆë ¨')
    parser.add_argument('--data', type=str, default='yolo_dataset/images',
                       help='ì´ë¯¸ì§€ í´ë” ê²½ë¡œ')
    parser.add_argument('--epochs', type=int, default=30,
                       help='í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch-size', type=int, default=4,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--lr', type=float, default=0.001,
                       help='í•™ìŠµë¥ ')
    parser.add_argument('--check-only', action='store_true',
                       help='ë°ì´í„°ì…‹ í™•ì¸ë§Œ ìˆ˜í–‰')
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸ¯ YOLO ëª¨ë¸ í›ˆë ¨ ì‹œìŠ¤í…œ")
    print("=" * 70)
    print(f"í´ë˜ìŠ¤: {', '.join(CLASSES)}")
    print(f"ë°ì´í„° í´ë”: {args.data}")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    setup_directories()
    
    # ë°ì´í„°ì…‹ í™•ì¸
    if not check_dataset(args.data):
        return
    
    if args.check_only:
        print("\nâœ… ë°ì´í„°ì…‹ í™•ì¸ ì™„ë£Œ!")
        return
    
    # í›ˆë ¨ ì„¤ì • ì—…ë°ì´íŠ¸
    config = TRAIN_CONFIG.copy()
    config['epochs'] = args.epochs
    config['batch_size'] = args.batch_size
    config['learning_rate'] = args.lr
    
    print(f"\nğŸ› ï¸ í›ˆë ¨ ì„¤ì •:")
    for key, value in config.items():
        print(f"   - {key}: {value}")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("\ní›ˆë ¨ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
    if response.lower() != 'y':
        print("í›ˆë ¨ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    try:
        # í›ˆë ¨ ì‹œì‘
        trainer = YOLOTrainer(args.data, config)
        trainer.train()
        
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()