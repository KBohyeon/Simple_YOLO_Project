

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import json
from pathlib import Path
from config import *

class SimpleYOLO(nn.Module):
    """YOLO ëª¨ë¸"""
    def __init__(self, num_classes=NUM_CLASSES, grid_size=7):
        super(SimpleYOLO, self).__init__()
        self.num_classes = num_classes
        self.grid_size = grid_size
        
        # CNN ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 112x112
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 56x56
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 28x28
            
            # Block 4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 14x14
            
            # Block 5
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 7x7
        )
        
        # YOLO í—¤ë“œ
        self.output_size = 5 + num_classes  # x, y, w, h, conf + classes
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, grid_size * grid_size * self.output_size)
        )
        
    def forward(self, x):
        batch_size = x.size(0)
        x = self.features(x)
        x = x.view(batch_size, -1)
        x = self.classifier(x)
        x = x.view(batch_size, self.grid_size, self.grid_size, self.output_size)
        return x

class YOLOLoss(nn.Module):
    """YOLO ì†ì‹¤ í•¨ìˆ˜"""
    def __init__(self, lambda_coord=5.0, lambda_noobj=0.5):
        super(YOLOLoss, self).__init__()
        self.lambda_coord = lambda_coord
        self.lambda_noobj = lambda_noobj
        
    def forward(self, predictions, targets):
        batch_size = predictions.size(0)
        
        pred_boxes = predictions[:, :, :, :4]
        pred_conf = predictions[:, :, :, 4:5]
        pred_class = predictions[:, :, :, 5:]
        
        target_boxes = targets[:, :, :, :4]
        target_conf = targets[:, :, :, 4:5]
        target_class = targets[:, :, :, 5:]
        
        obj_mask = target_conf.squeeze(-1) > 0
        noobj_mask = target_conf.squeeze(-1) == 0
        
        # ì¢Œí‘œ ì†ì‹¤
        if obj_mask.sum() > 0:
            coord_loss = self.lambda_coord * F.mse_loss(
                pred_boxes[obj_mask], target_boxes[obj_mask], reduction='sum'
            )
        else:
            coord_loss = torch.tensor(0.0, device=predictions.device)
        
        # ê°ì²´ confidence ì†ì‹¤
        if obj_mask.sum() > 0:
            obj_conf_loss = F.mse_loss(
                pred_conf[obj_mask], target_conf[obj_mask], reduction='sum'
            )
        else:
            obj_conf_loss = torch.tensor(0.0, device=predictions.device)
        
        # ê°ì²´ ì—†ëŠ” ì…€ confidence ì†ì‹¤
        if noobj_mask.sum() > 0:
            noobj_conf_loss = self.lambda_noobj * F.mse_loss(
                pred_conf[noobj_mask], target_conf[noobj_mask], reduction='sum'
            )
        else:
            noobj_conf_loss = torch.tensor(0.0, device=predictions.device)
        
        # í´ë˜ìŠ¤ ì†ì‹¤
        if obj_mask.sum() > 0:
            class_loss = F.mse_loss(
                pred_class[obj_mask], target_class[obj_mask], reduction='sum'
            )
        else:
            class_loss = torch.tensor(0.0, device=predictions.device)
        
        total_loss = coord_loss + obj_conf_loss + noobj_conf_loss + class_loss
        return total_loss / batch_size

class RealImageDataset(Dataset):
    """ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ì…‹"""
    def __init__(self, image_folder, grid_size=7, img_size=224, transform=None):
        self.image_folder = Path(image_folder)
        self.grid_size = grid_size
        self.img_size = img_size
        self.transform = transform
        
        # ì´ë¯¸ì§€ íŒŒì¼ê³¼ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì°¾ê¸°
        self.data_pairs = []
        extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
        
        # annotations í´ë” ê²½ë¡œ
        from config import PATHS
        annotations_dir = PATHS['annotations_dir']
        
        for ext in extensions:
            for img_path in self.image_folder.glob(ext):
                # annotations í´ë”ì—ì„œ JSON íŒŒì¼ ì°¾ê¸°
                annotation_path = annotations_dir / (img_path.stem + '.json')
                if annotation_path.exists():
                    self.data_pairs.append((img_path, annotation_path))
        
        print(f"ì´ {len(self.data_pairs)}ê°œì˜ ë¼ë²¨ë§ëœ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        img_path, annotation_path = self.data_pairs[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(img_path).convert('RGB')
        orig_width, orig_height = image.size
        
        # ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ
        with open(annotation_path, 'r', encoding='utf-8') as f:  # â† encoding ì¶”ê°€!
            data = json.load(f)
        
        annotations = data['annotations']
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        image = image.resize((self.img_size, self.img_size))
        
        # YOLO íƒ€ê²Ÿ ìƒì„±
        target = self.create_yolo_target(annotations, orig_width, orig_height)
        
        if self.transform:
            image = self.transform(image)
        
        return image, target
    
    def create_yolo_target(self, annotations, orig_width, orig_height):
        """ì–´ë…¸í…Œì´ì…˜ì„ YOLO íƒ€ê²Ÿ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        target = torch.zeros(self.grid_size, self.grid_size, 5 + NUM_CLASSES)
        
        for ann in annotations:
            class_id = ann['class_id']
            x1, y1, x2, y2 = ann['bbox']
            
            # ì •ê·œí™”ëœ ì¢Œí‘œ
            x_center = ((x1 + x2) / 2) / orig_width
            y_center = ((y1 + y2) / 2) / orig_height
            width = (x2 - x1) / orig_width
            height = (y2 - y1) / orig_height
            
            # ê·¸ë¦¬ë“œ ì…€ ì°¾ê¸°
            grid_x = int(x_center * self.grid_size)
            grid_y = int(y_center * self.grid_size)
            
            grid_x = min(grid_x, self.grid_size - 1)
            grid_y = min(grid_y, self.grid_size - 1)
            
            # ì´ë¯¸ ê°ì²´ê°€ ìˆëŠ” ì…€ì´ë©´ ìŠ¤í‚µ
            if target[grid_y, grid_x, 4] > 0:
                continue
            
            # ì…€ ë‚´ ìƒëŒ€ ìœ„ì¹˜
            x_offset = (x_center * self.grid_size) - grid_x
            y_offset = (y_center * self.grid_size) - grid_y
            
            # íƒ€ê²Ÿ ì„¤ì •
            target[grid_y, grid_x, 0] = x_offset
            target[grid_y, grid_x, 1] = y_offset
            target[grid_y, grid_x, 2] = width
            target[grid_y, grid_x, 3] = height
            target[grid_y, grid_x, 4] = 1.0  # confidence
            target[grid_y, grid_x, 5 + class_id] = 1.0  # class
        
        return target

def get_transform(train=True):
    """ë°ì´í„° ë³€í™˜ í•¨ìˆ˜"""
    if train:
        # í›ˆë ¨ìš© (ë°ì´í„° ì¦ê°• í¬í•¨)
        transform = transforms.Compose([
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomHorizontalFlip(0.5),
            transforms.ToTensor(),
            transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)
        ])
    else:
        # í…ŒìŠ¤íŠ¸ìš©
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)
        ])
    
    return transform

def decode_predictions(prediction, confidence_threshold=0.3):
    """YOLO ì¶œë ¥ì„ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ë””ì½”ë”©"""
    detections = []
    grid_size = prediction.size(0)
    
    for i in range(grid_size):
        for j in range(grid_size):
            cell_pred = prediction[i, j]
            
            x = cell_pred[0].item()
            y = cell_pred[1].item()
            w = cell_pred[2].item()
            h = cell_pred[3].item()
            confidence = torch.sigmoid(cell_pred[4]).item()
            
            if confidence > confidence_threshold:
                class_probs = torch.softmax(cell_pred[5:], dim=0)
                class_id = torch.argmax(class_probs).item()
                class_confidence = class_probs[class_id].item()
                
                final_confidence = confidence * class_confidence
                
                if final_confidence > confidence_threshold:
                    abs_x = (x + j) / grid_size
                    abs_y = (y + i) / grid_size
                    
                    detections.append([abs_x, abs_y, abs(w), abs(h), 
                                     final_confidence, class_id])
    
    return detections

def non_max_suppression(boxes, scores, iou_threshold=0.5):
    """Non-Maximum Suppression"""
    if len(boxes) == 0:
        return [], []
    
    boxes = torch.tensor(boxes) if not isinstance(boxes, torch.Tensor) else boxes
    scores = torch.tensor(scores) if not isinstance(scores, torch.Tensor) else scores
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    indices = torch.argsort(scores, descending=True)
    
    keep = []
    while len(indices) > 0:
        current = indices[0]
        keep.append(current.item())
        
        if len(indices) == 1:
            break
        
        # IoU ê³„ì‚°
        current_box = boxes[current].unsqueeze(0)
        other_boxes = boxes[indices[1:]]
        
        ious = calculate_iou(current_box, other_boxes)
        
        # IoUê°€ ì„ê³„ê°’ ë¯¸ë§Œì¸ ë°•ìŠ¤ë“¤ë§Œ ìœ ì§€
        indices = indices[1:][ious.squeeze() < iou_threshold]
    
    return boxes[keep], scores[keep]

def calculate_iou(box1, box2):
    """IoU (Intersection over Union) ê³„ì‚°"""
    # box format: (x_center, y_center, width, height)
    
    # ì¢Œìƒë‹¨, ìš°í•˜ë‹¨ ì¢Œí‘œë¡œ ë³€í™˜
    box1_x1 = box1[:, 0] - box1[:, 2] / 2
    box1_y1 = box1[:, 1] - box1[:, 3] / 2
    box1_x2 = box1[:, 0] + box1[:, 2] / 2
    box1_y2 = box1[:, 1] + box1[:, 3] / 2
    
    box2_x1 = box2[:, 0] - box2[:, 2] / 2
    box2_y1 = box2[:, 1] - box2[:, 3] / 2
    box2_x2 = box2[:, 0] + box2[:, 2] / 2
    box2_y2 = box2[:, 1] + box2[:, 3] / 2
    
    # êµì§‘í•© ì˜ì—­
    inter_x1 = torch.max(box1_x1.unsqueeze(1), box2_x1.unsqueeze(0))
    inter_y1 = torch.max(box1_y1.unsqueeze(1), box2_y1.unsqueeze(0))
    inter_x2 = torch.min(box1_x2.unsqueeze(1), box2_x2.unsqueeze(0))
    inter_y2 = torch.min(box1_y2.unsqueeze(1), box2_y2.unsqueeze(0))
    
    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    
    # ê° ë°•ìŠ¤ì˜ ë©´ì 
    box1_area = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)
    box2_area = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)
    
    # í•©ì§‘í•© ë©´ì 
    union_area = box1_area.unsqueeze(1) + box2_area.unsqueeze(0) - inter_area
    
    iou = inter_area / union_area
    return iou

def visualize_predictions(image, detections, save_path=None, show_plot=True):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    fig, ax = plt.subplots(1, figsize=(12, 8))
    ax.imshow(image)
    
    for detection in detections:
        x, y, w, h = detection['bbox']
        confidence = detection['confidence']
        class_name = detection['class']
        class_id = detection['class_id']
        
        # ì¤‘ì‹¬ì  -> ì¢Œìƒë‹¨ ë³€í™˜
        x1 = x - w/2
        y1 = y - h/2
        
        color = COLORS[class_id % len(COLORS)]
        
        rect = patches.Rectangle((x1, y1), w, h, 
                               linewidth=3, 
                               edgecolor=color, 
                               facecolor='none')
        ax.add_patch(rect)
        
        label = f'{class_name}: {confidence:.2f}'
        ax.text(x1, y1-10, label, 
                bbox=dict(boxstyle="round,pad=0.3", 
                         facecolor=color, 
                         alpha=0.8),
                fontsize=12, color='white', weight='bold')
    
    ax.set_xlim(0, image.size[0])
    ax.set_ylim(image.size[1], 0)
    ax.axis('off')
    ax.set_title('YOLO ê°ì²´ ê²€ì¶œ ê²°ê³¼', fontsize=16)
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=150)
        print(f"ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {save_path}")
    
    if show_plot:
        plt.show()
    else:
        plt.close()

def load_model(model_path, device=DEVICE):
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    model = SimpleYOLO(num_classes=NUM_CLASSES).to(device)
    
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    print(f"   - í›ˆë ¨ ì—í¬í¬: {checkpoint.get('epoch', 'Unknown')}")
    print(f"   - ìµœì¢… ì†ì‹¤: {checkpoint.get('loss', 'Unknown'):.4f}")
    
    return model

def save_model(model, optimizer, epoch, loss, save_path):
    """ëª¨ë¸ ì €ì¥"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'classes': CLASSES,
        'model_config': MODEL_CONFIG
    }, save_path)
    
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")

if __name__ == "__main__":
    print("ğŸ§  YOLO ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ")
    print(f"í´ë˜ìŠ¤: {CLASSES}")
    print(f"ë””ë°”ì´ìŠ¤: {DEVICE}")
    
    # í…ŒìŠ¤íŠ¸
    model = SimpleYOLO()
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")